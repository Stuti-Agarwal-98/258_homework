{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autograd.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-AaXjOg3v5c"
      },
      "source": [
        "#Loading the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK7i1STMPrRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a030ece4-665e-4a48-c2db-c8e3b9da7f97"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "# Load MNIST dataset from keras\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# Number of samples\n",
        "n = 1000\n",
        "random_index = np.random.randint(n, size=n) # Randomly select training data\n",
        "images, labels = (X_train[random_index].reshape(len(X_train[random_index]),28*28)/255, Y_train[random_index])\n",
        "\n",
        "# Convert train label into one-hot encoding\n",
        "train_labels = np.zeros((len(labels),10))\n",
        "for i,l in enumerate(labels):\n",
        "    train_labels[i][l] = 1\n",
        "\n",
        "# Flatten the testing data \n",
        "test_images = X_test.reshape(len(X_test),28*28)/255\n",
        "\n",
        "test_labels = np.zeros((len(Y_test),10)) # one-hot encode test labels\n",
        "for i,l in enumerate(Y_test):\n",
        "    test_labels[i][l] = 1\n",
        "\n",
        "print(f'Train Data: {images.shape}, Train Label: {train_labels.shape}, Test Data: {test_images.shape}, Test Label: {test_labels.shape}')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data: (1000, 784), Train Label: (1000, 10), Test Data: (10000, 784), Test Label: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3YP1C1u333t"
      },
      "source": [
        "#Tensor class with autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGzVVMTi2zgx"
      },
      "source": [
        "class Tensor (object):\n",
        "    \n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "        \n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,100000)\n",
        "        else:\n",
        "            self.id = id\n",
        "        \n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "        \n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True \n",
        "        \n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        " \n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "          \n",
        "            assert grad.autograd == False\n",
        "\n",
        "            if(self.creators is not None and \n",
        "               (self.all_children_grads_accounted_for() or \n",
        "                grad_origin is None)):\n",
        "\n",
        "                if(self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "                    \n",
        "                if(self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if(self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)                    \n",
        "                    \n",
        "                if(self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "                    \n",
        "                if(self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if(\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,\n",
        "                                                               self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if(\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "                    \n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "                    \n",
        "                if(self.creation_op == \"sigmoid\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
        "                \n",
        "                if(self.creation_op == \"tanh\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
        "                \n",
        "                if(self.creation_op == \"index_select\"):\n",
        "                    new_grad = np.zeros_like(self.creators[0].data)\n",
        "                    indices_ = self.index_select_indices.data.flatten()\n",
        "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
        "                    for i in range(len(indices_)):\n",
        "                        new_grad[indices_[i]] += grad_[i]\n",
        "                    self.creators[0].backward(Tensor(new_grad))\n",
        "                    \n",
        "                if(self.creation_op == \"cross_entropy\"):\n",
        "                    dx = self.softmax_output - self.target_dist\n",
        "                    self.creators[0].backward(Tensor(dx))\n",
        "                    \n",
        "    def __add__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "    \n",
        "    def __sub__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)    \n",
        "\n",
        "    def sum(self, dim):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "    \n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "        \n",
        "        if(self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "    \n",
        "    def transpose(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "        \n",
        "        return Tensor(self.data.transpose())\n",
        "    \n",
        "    def mm(self, x):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "    \n",
        "    def sigmoid(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sigmoid\")\n",
        "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
        "\n",
        "    def tanh(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(np.tanh(self.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"tanh\")\n",
        "        return Tensor(np.tanh(self.data))\n",
        "    \n",
        "    def index_select(self, indices):\n",
        "\n",
        "        if(self.autograd):\n",
        "            new = Tensor(self.data[indices.data],\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"index_select\")\n",
        "            new.index_select_indices = indices\n",
        "            return new\n",
        "        return Tensor(self.data[indices.data])\n",
        "   \n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "    \n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJF8Mx9e3-dR"
      },
      "source": [
        "#automatic optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObJOVL9Q23x2"
      },
      "source": [
        "class SGD(object):\n",
        "    \n",
        "    def __init__(self, parameters, alpha=0.1):\n",
        "        self.parameters = parameters\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def zero(self):\n",
        "        for p in self.parameters:\n",
        "            p.grad.data *= 0\n",
        "        \n",
        "    def step(self, zero=True):\n",
        "        \n",
        "        for p in self.parameters:\n",
        "            \n",
        "            p.data -= p.grad.data * self.alpha\n",
        "            \n",
        "            if(zero):\n",
        "                p.grad.data *= 0"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX0gvJkk4FQb"
      },
      "source": [
        "#sequential linear layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWv0MyYZ28bq"
      },
      "source": [
        "class Layer(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.parameters = list()\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super().__init__()\n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "        self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
        "\n",
        "class Sequential(Layer):\n",
        "    \n",
        "    def __init__(self, layers=list()):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = layers\n",
        "    \n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        params = list()\n",
        "        for l in self.layers:\n",
        "            params += l.get_parameters()\n",
        "        return params"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCs8bNpO4K9W"
      },
      "source": [
        "#activation layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxiiNEu-29O4"
      },
      "source": [
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "    \n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0drmDc34NhD"
      },
      "source": [
        "#loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PWpEUfn3A9F"
      },
      "source": [
        "class MSELoss(Layer):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        return ((pred - target)*(pred - target)).sum(0)\n",
        "        #return ((pred - target).data ** 2).mean()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RkMw09pzpM9"
      },
      "source": [
        "#Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIZCQ97n3P4L",
        "outputId": "60106d77-3977-4fef-a26d-9aa21602d129"
      },
      "source": [
        "\n",
        "x = Tensor([1,2,3,4,5], autograd=True)\n",
        "y = Tensor([2,2,2,2,2], autograd=True) \n",
        "z = x + y\n",
        "z.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.creators)\n",
        "print(z.creation_op)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1]\n",
            "[1 1 1 1 1]\n",
            "[array([1, 2, 3, 4, 5]), array([2, 2, 2, 2, 2])]\n",
            "add\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJLCanAX3VM9",
        "outputId": "f308e92b-64ea-4d04-b016-04de3cd7d63d"
      },
      "source": [
        "\n",
        "a = Tensor([1,2,3,4,5], autograd=True)\n",
        "b = Tensor([2,2,2,2,2], autograd=True)\n",
        "c = Tensor([5,4,3,2,1], autograd=True)\n",
        "\n",
        "d = a + b\n",
        "e = b + c\n",
        "f = d + e\n",
        "\n",
        "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "\n",
        "print(b.grad.data == np.array([2,2,2,2,2]))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXrI4vRy3V26",
        "outputId": "b829517b-c048-4a30-b6dd-754d49af4177"
      },
      "source": [
        "a = Tensor([1,2,3,4,5], autograd=True)\n",
        "b = Tensor([2,2,2,2,2], autograd=True)\n",
        "c = Tensor([5,4,3,2,1], autograd=True)\n",
        "\n",
        "d = a + (-b)\n",
        "e = (-b) + c\n",
        "f = d + e\n",
        "\n",
        "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "\n",
        "print(b.grad.data == np.array([-2,-2,-2,-2,-2]))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFgYvmS_4Vq0"
      },
      "source": [
        "#checksum,expand and transpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAqRDhoz3YZo",
        "outputId": "a5610c83-4b2b-4a1e-c3a4-152336dd03b2"
      },
      "source": [
        "x = Tensor(np.array([[1,2,3],\n",
        "                     [4,5,6]]))\n",
        "x_t = x.transpose()\n",
        "print(x.sum(0).data==[5, 7, 9], x.sum(1).data==[ 6, 15])\n",
        "print()\n",
        "print(x_t)\n",
        "print()\n",
        "print(x.expand(dim=2, copies=4))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True] [ True  True]\n",
            "\n",
            "[[1 4]\n",
            " [2 5]\n",
            " [3 6]]\n",
            "\n",
            "[[[1 1 1 1]\n",
            "  [2 2 2 2]\n",
            "  [3 3 3 3]]\n",
            "\n",
            " [[4 4 4 4]\n",
            "  [5 5 5 5]\n",
            "  [6 6 6 6]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdWavBNX4baD"
      },
      "source": [
        "#minibatch traning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHmkuN7y3bjQ"
      },
      "source": [
        "\n",
        "def train_on_batch(x, y):\n",
        "  x, y = Tensor(x,autograd=True), Tensor(y,autograd=True)\n",
        "  y_pred = model.forward(x)\n",
        "  loss = criterion.forward(y_pred, y)\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  return model, loss"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty0FNXb_3eHc",
        "outputId": "60199231-74f8-4ade-e1e8-b1030bd6f974"
      },
      "source": [
        "data = Tensor(images, autograd=True)\n",
        "target = Tensor(train_labels, autograd=True)\n",
        "\n",
        "model = Sequential([Linear(784,200), Tanh() ,Linear(200,80), Tanh(), Linear(80,10), Sigmoid()])\n",
        "criterion = MSELoss()\n",
        "\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=0.001)\n",
        "batch_size = 256\n",
        "for epoch in range(500):\n",
        "    for i in range(int(len(data.data)/batch_size)):\n",
        "      batch_start, batch_end = ((i * batch_size),((i+1)*batch_size))\n",
        "      model, loss = train_on_batch(data.data[batch_start:batch_end], target.data[batch_start:batch_end])\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        pred = model.forward(data)\n",
        "        train_correct = (np.argmax(target.data,axis=1) == np.argmax(pred.data,axis=1)).mean()\n",
        "        print(\"Train Accuracy:\",train_correct,\"Train Loss:\", (loss.sum(0).data)/len(data.data))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.329 Train Loss: 0.22216348707571257\n",
            "Train Accuracy: 0.952 Train Loss: 0.02597204681394431\n",
            "Train Accuracy: 0.973 Train Loss: 0.011306361154874612\n",
            "Train Accuracy: 0.977 Train Loss: 0.0073477530547766436\n",
            "Train Accuracy: 0.978 Train Loss: 0.005713554653718306\n",
            "Train Accuracy: 0.978 Train Loss: 0.004875580997684626\n",
            "Train Accuracy: 0.978 Train Loss: 0.004376545299414445\n",
            "Train Accuracy: 0.978 Train Loss: 0.003978981488713593\n",
            "Train Accuracy: 0.98 Train Loss: 0.00338266421627842\n",
            "Train Accuracy: 0.98 Train Loss: 0.0029356025622354967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU1wOqOF4kyl"
      },
      "source": [
        "#visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTUmrqbk3hia"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# View the data\n",
        "def visualize_N_elems_of_dataset(dataset, N, name):\n",
        "  print(\"Visualizing the \" + name + \" dataset.\")\n",
        "  fig = plt.figure(figsize=plt.figaspect(0.3))\n",
        "\n",
        "  # Plot N elems of the dataset\n",
        "  for image in range(0, N):\n",
        "    ax = fig.add_subplot(1, 10, image+1)\n",
        "    ax.imshow(dataset[image], cmap='Accent')"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "y_KoUZ883m_a",
        "outputId": "cbe8b435-f14d-431c-c7a4-7899e308dfa7"
      },
      "source": [
        "\n",
        "N = 10\n",
        "visualize_N_elems_of_dataset(X_train, N, \"train\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Visualizing the train dataset.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 960x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}